{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import mysql.connector\n",
    "from enum import Enum\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "import typing\n",
    "\n",
    "import schema_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def append_JSON_obects(filename: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns all JSON objects in file appended to a single list\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): The name of the file containing the JSON objects\n",
    "    \n",
    "    Returns:\n",
    "        branch_list (list[str]): The list of json obects\n",
    "    \n",
    "    \"\"\"\n",
    "    branch_list = []\n",
    "\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for i in data:\n",
    "        branch_list.append(i)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return branch_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_directory(directoryName: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a dictionary of JSON file names within the given directory\n",
    "\n",
    "    Parameters: \n",
    "        directoryName (str): the name of a directory containing only JSON files\n",
    "\n",
    "    Returns:\n",
    "        parsed_list (list[str]): list of file names within the directory\n",
    "    \"\"\"\n",
    "    parsed_list = {}\n",
    "    for filename in os.listdir(directoryName):\n",
    "        f = os.path.join(directoryName, filename)\n",
    "        if os.path.isfile(f):\n",
    "            table_name = filename.split(\".\")[0].upper()\n",
    "            parsed_list[table_name] = append_JSON_obects(f)\n",
    "        else:\n",
    "            print(\"Unable to find\", f)\n",
    "    return parsed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TBL_NAME(Enum):\n",
    "    CREDIT = \"CDW_SAPP_CREDIT\"\n",
    "    BRANCH = \"CDW_SAPP_BRANCH\"\n",
    "    CUST = \"CDW_SAPP_CUSTOMER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customer data table not have an area code. It appears almost every phone number in the branch database uses 123 - this appears to be a placeholder value, so I'm just applying it to the customer table as well.\n",
    "\n",
    "## STrecth goal: IMPORT DATASET FOR AREA CODE ZIP CODE/ADDRESS TO PUT REAL VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDW_SAPP_CREDIT DataFrame[BRANCH_CODE: bigint, CREDIT_CARD_NO: string, CUST_SSN: bigint, DAY: bigint, MONTH: bigint, TRANSACTION_ID: bigint, TRANSACTION_TYPE: string, TRANSACTION_VALUE: double, YEAR: bigint]\n",
      "CDW_SAPP_BRANCH DataFrame[BRANCH_CITY: string, BRANCH_CODE: bigint, BRANCH_NAME: string, BRANCH_PHONE: string, BRANCH_STATE: string, BRANCH_STREET: string, BRANCH_ZIP: bigint, LAST_UPDATED: string]\n",
      "CDW_SAPP_CUSTOMER DataFrame[APT_NO: string, CREDIT_CARD_NO: string, CUST_CITY: string, CUST_COUNTRY: string, CUST_EMAIL: string, CUST_PHONE: bigint, CUST_STATE: string, CUST_ZIP: string, FIRST_NAME: string, LAST_NAME: string, LAST_UPDATED: string, MIDDLE_NAME: string, SSN: bigint, STREET_NAME: string]\n"
     ]
    }
   ],
   "source": [
    "# create the SparkSession\n",
    "spark = SparkSession.builder.appName('Bank_Analysis').getOrCreate()\n",
    "\n",
    "parsed_lists = extract_from_directory(\"json\")\n",
    "df_dict = {}\n",
    "for name in parsed_lists:\n",
    "    df_dict[name] = spark.createDataFrame(parsed_lists[name])\n",
    "\n",
    "\n",
    "for k,v in df_dict.items():\n",
    "    print(k,v)\n",
    "    df_dict[k].createOrReplaceTempView(k)\n",
    "    temp = spark.sql(schema_patterns.pattern_dict[k])\n",
    "    df_dict[k] = temp\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rdd_customer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mrdd_customer\u001b[49m\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(TBL_NAME\u001b[38;5;241m.\u001b[39mCUST\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[1;32m      5\u001b[0m df_customer \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(schema_patterns\u001b[38;5;241m.\u001b[39mpattern_dict[TBL_NAME\u001b[38;5;241m.\u001b[39mCUST\u001b[38;5;241m.\u001b[39mvalue])\n\u001b[1;32m      7\u001b[0m rdd_branch\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(TBL_NAME\u001b[38;5;241m.\u001b[39mBRANCH\u001b[38;5;241m.\u001b[39mvalue)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rdd_customer' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/14 16:56:59 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
